{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Modal Rag\n",
    "\n",
    "**Note**: The [GPT-4V model by OpenAI](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo#:~:text=to%20Apr%202023-,gpt%2D4%2Dvision%2Dpreview,-GPT%2D4%20Turbo) is still in preview.\n",
    "\n",
    "This example will demonstrate how to preform [RAG](https://arxiv.org/abs/2005.11401) on images, using the new GPT-4V model by OpenAI.\n",
    "\n",
    "At a high level we're:\n",
    "\n",
    "- Passing all images to GPT-4V and summarizing their contents.\n",
    "- Embedding the summaries and adding links to the images in metadata.\n",
    "- Using semantic search on a query to retrieve the most relevant image.\n",
    "- Passing the full image and user query to GPT-4V for a final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deno.env.set(\"OPENAI_API_KEY\", \"\");\n",
    "\n",
    "import { ChatOpenAI } from \"npm:langchain@0.0.185/chat_models/openai\";\n",
    "import { Document } from \"npm:langchain@0.0.185/document\";\n",
    "import { OpenAIEmbeddings } from \"npm:langchain@0.0.185/embeddings/openai\";\n",
    "import { ChatPromptTemplate } from \"npm:langchain@0.0.185/prompts\";\n",
    "import { HumanMessage } from \"npm:langchain@0.0.185/schema\";\n",
    "import { StringOutputParser } from \"npm:langchain@0.0.185/schema/output_parser\";\n",
    "import { RunnableSequence } from \"npm:langchain@0.0.185/schema/runnable\";\n",
    "import { HNSWLib } from \"npm:langchain@0.0.185/vectorstores/hnswlib\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate `ChatOpenAI` using the vision model and load in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const model = new ChatOpenAI({\n",
    "  modelName: \"gpt-4-vision-preview\",\n",
    "  maxTokens: 1024,\n",
    "}).pipe(new StringOutputParser());\n",
    "// Load in images\n",
    "const usNationalDebt = await Deno.readFile(\n",
    "  \"../examples/multi_modal_content/us_national_debt_chart.jpg\"\n",
    ");\n",
    "const canadianNationalDebt = await Deno.readFile(\n",
    "  \"../examples/multi_modal_content/canadian_debt_by_gdp.jpg\"\n",
    ");\n",
    "const mexicanNationalDebt = await Deno.readFile(\n",
    "  \"../examples/multi_modal_content/mexico_national_debt_monthly.jpg\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dict containing all the images. This will be helpful later on when we want to retrieve a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const imageDict = {\n",
    "  us: usNationalDebt,\n",
    "  canada: canadianNationalDebt,\n",
    "  mexico: mexicanNationalDebt,\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Map over each image in the dict and create a prompt message, encoding the image in base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const promptMessages = Object.keys(imageDict).map(\n",
    "  (key) =>\n",
    "    new HumanMessage({\n",
    "      content: [\n",
    "        {\n",
    "          type: \"text\",\n",
    "          text: \"Describe the contents of this image in detail.\",\n",
    "        },\n",
    "        {\n",
    "          type: \"image_url\",\n",
    "          image_url: {\n",
    "            url: `data:image/jpeg;base64,${imageDict[\n",
    "              key as keyof typeof imageDict\n",
    "            ].toString(\"base64\")}`,\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    })\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the model to generate a summary of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const summaries = await Promise.all([\n",
    "  model.invoke([promptMessages[0]]),\n",
    "  model.invoke([promptMessages[1]]),\n",
    "  model.invoke([promptMessages[2]]),\n",
    "]);\n",
    "console.log(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the summaries\n",
    "\n",
    "Create a document for each summary, also including the image dict key as metadata so we can retrieve the actual image later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const documents = summaries.map(\n",
    "  (summary, i) =>\n",
    "    new Document({\n",
    "      pageContent: summary,\n",
    "      metadata: {\n",
    "        imageKey: Object.keys(imageDict)[i],\n",
    "      },\n",
    "    })\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the vector store with `OpenAIEmbeddings` and the documents we created above. Then, instantiate the store as a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const vectorStore = await HNSWLib.fromDocuments(\n",
    "  documents,\n",
    "  new OpenAIEmbeddings()\n",
    ");\n",
    "const retriever = vectorStore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "Create a `HumanMessage` prompt which will contain the image with the relevant content based on the users question. Since we do not know the image yet we use an input variable `{imageString}` which we'll replace with the base 64 encoded image at runtime.\n",
    "\n",
    "Then, create a `ChatPromptTemplate` with the `imageMessage` and an input variable for the users question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const imageMessage = new HumanMessage({\n",
    "  content: [\n",
    "    {\n",
    "      type: \"image_url\",\n",
    "      image_url: {\n",
    "        url: \"data:image/jpeg;base64,{imageString}\",\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"ai\", \"Answer the users question using the provided image.\"],\n",
    "  [\"human\", \"{question}\"],\n",
    "  imageMessage,\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the chain.\n",
    "\n",
    "Here we're taking in a single input which is the users question, then preforming a similarity search to find the most relevant document, and using the first returned doc since in our case we know only 1 document will match the question.\n",
    "\n",
    "Then, using the image key in the metadata we're able to retrieve the relevant image and encode it to then be passed into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = RunnableSequence.from([\n",
    "  async (input: string) => {\n",
    "    const relevantDoc = (await retriever.getRelevantDocuments(input))[0];\n",
    "    const imageKey = relevantDoc.metadata.imageKey as keyof typeof imageDict;\n",
    "    const imageString = imageDict[imageKey].toString(\"base64\");\n",
    "    return {\n",
    "      imageString,\n",
    "      question: input,\n",
    "    };\n",
    "  },\n",
    "  prompt,\n",
    "  model,\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, invoke the model and sit back while the magic happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const response = await chain.invoke(\n",
    "  \"How much was Mexico's national debt increasing by on a monthly basis?\"\n",
    ");\n",
    "console.log(\"response\\n\", response);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
