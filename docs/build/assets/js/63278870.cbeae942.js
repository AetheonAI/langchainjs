"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4571],{5318:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>g});var a=n(7378);function s(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){s(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,s=function(e,t){if(null==e)return{};var n,a,s={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(s[n]=e[n]);return s}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var i=a.createContext({}),p=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(i.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,s=e.mdxType,r=e.originalType,i=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),h=s,g=m["".concat(i,".").concat(h)]||m[h]||u[h]||r;return n?a.createElement(g,o(o({ref:t},c),{},{components:n})):a.createElement(g,o({ref:t},c))}));function g(e,t){var n=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var r=n.length,o=new Array(r);o[0]=h;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l[m]="string"==typeof e?e:s,o[1]=l;for(var p=2;p<r;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},2934:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>i,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var a=n(5773),s=(n(7378),n(5318));const r={sidebar_position:3},o="Quickstart, using Chat Models",l={unversionedId:"getting-started/guide-chat",id:"getting-started/guide-chat",title:"Quickstart, using Chat Models",description:"Chat models are a variation on language models.",source:"@site/docs/getting-started/guide-chat.mdx",sourceDirName:"getting-started",slug:"/getting-started/guide-chat",permalink:"/langchainjs/docs/getting-started/guide-chat",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/docs/getting-started/guide-chat.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"sidebar",previous:{title:"Quickstart, using LLMs",permalink:"/langchainjs/docs/getting-started/guide-llm"},next:{title:"Schema",permalink:"/langchainjs/docs/modules/schema/"}},i={},p=[{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Chat Models: Message in, Message out",id:"chat-models-message-in-message-out",level:3},{value:"Multiple Messages",id:"multiple-messages",level:4},{value:"Multiple Completions",id:"multiple-completions",level:4},{value:"Chat Prompt Templates: Manage Prompts for Chat Models",id:"chat-prompt-templates-manage-prompts-for-chat-models",level:3},{value:"Model + Prompt = LLMChain",id:"model--prompt--llmchain",level:3},{value:"Agents: Dynamically Run Chains Based on User Input",id:"agents-dynamically-run-chains-based-on-user-input",level:3},{value:"Memory: Add State to Chains and Agents",id:"memory-add-state-to-chains-and-agents",level:3},{value:"Streaming",id:"streaming",level:2}],c={toc:p},m="wrapper";function u(e){let{components:t,...n}=e;return(0,s.kt)(m,(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"quickstart-using-chat-models"},"Quickstart, using Chat Models"),(0,s.kt)("p",null,'Chat models are a variation on language models.\nWhile chat models use language models under the hood, the interface they expose is a bit different.\nRather than expose a "text in, text out" API, they expose an interface where "chat messages" are the inputs and outputs.'),(0,s.kt)("p",null,"Chat model APIs are fairly new, so we are still figuring out the correct abstractions."),(0,s.kt)("h2",{id:"installation-and-setup"},"Installation and Setup"),(0,s.kt)("p",null,"To get started, follow the ",(0,s.kt)("a",{parentName:"p",href:"./install"},"installation instructions")," to install LangChain."),(0,s.kt)("h2",{id:"getting-started"},"Getting Started"),(0,s.kt)("p",null,"This section covers how to get started with chat models. The interface is based around messages rather than raw text."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'import { ChatOpenAI } from "langchain/chat_models";\nimport { HumanChatMessage, SystemChatMessage } from "langchain/schema";\n\nconst chat = new ChatOpenAI({ temperature: 0 });\n')),(0,s.kt)("p",null,"Here we create a chat model using the API key stored in the environment variable ",(0,s.kt)("inlineCode",{parentName:"p"},"OPENAI_API_KEY"),". We'll be calling this chat model throughout this section."),(0,s.kt)("h3",{id:"chat-models-message-in-message-out"},"Chat Models: Message in, Message out"),(0,s.kt)("p",null,"You can get chat completions by passing one or more messages to the chat model. The response will also be a message. The types of messages currently supported in LangChain are ",(0,s.kt)("inlineCode",{parentName:"p"},"AIChatMessage"),", ",(0,s.kt)("inlineCode",{parentName:"p"},"HumanChatMessage"),", ",(0,s.kt)("inlineCode",{parentName:"p"},"SystemChatMessage"),", and a generic ",(0,s.kt)("inlineCode",{parentName:"p"},"ChatMessage")," -- ChatMessage takes in an arbitrary role parameter, which we won't be using here. Most of the time, you'll just be dealing with ",(0,s.kt)("inlineCode",{parentName:"p"},"HumanChatMessage"),", ",(0,s.kt)("inlineCode",{parentName:"p"},"AIChatMessage"),", and ",(0,s.kt)("inlineCode",{parentName:"p"},"SystemChatMessage"),"."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'const response = await chat.call([\n  new HumanChatMessage(\n    "Translate this sentence from English to French. I love programming."\n  ),\n]);\n\nconsole.log(response);\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'AIChatMessage { text: "J\'aime programmer." }\n')),(0,s.kt)("h4",{id:"multiple-messages"},"Multiple Messages"),(0,s.kt)("p",null,"OpenAI's chat-based models (currently ",(0,s.kt)("inlineCode",{parentName:"p"},"gpt-3.5-turbo")," and ",(0,s.kt)("inlineCode",{parentName:"p"},"gpt-4"),") support multiple messages as input. See ",(0,s.kt)("a",{parentName:"p",href:"https://platform.openai.com/docs/guides/chat/chat-vs-completions"},"here")," for more information. Here is an example of sending a system and user message to the chat model:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'response = await chat.call([\n  new SystemChatMessage(\n    "You are a helpful assistant that translates English to French."\n  ),\n  new HumanChatMessage("Translate: I love programming."),\n]);\n\nconsole.log(response);\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'AIChatMessage { text: "J\'aime programmer." }\n')),(0,s.kt)("h4",{id:"multiple-completions"},"Multiple Completions"),(0,s.kt)("p",null,"You can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'const responseC = await chat.generate([\n  [\n    new SystemChatMessage(\n      "You are a helpful assistant that translates English to French."\n    ),\n    new HumanChatMessage(\n      "Translate this sentence from English to French. I love programming."\n    ),\n  ],\n  [\n    new SystemChatMessage(\n      "You are a helpful assistant that translates English to French."\n    ),\n    new HumanChatMessage(\n      "Translate this sentence from English to French. I love artificial intelligence."\n    ),\n  ],\n]);\n\nconsole.log(responseC);\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'{\n  generations: [\n    [\n      {\n        text: "J\'aime programmer.",\n        message: AIChatMessage { text: "J\'aime programmer." },\n      }\n    ],\n    [\n      {\n        text: "J\'aime l\'intelligence artificielle.",\n        message: AIChatMessage { text: "J\'aime l\'intelligence artificielle." }\n      }\n    ]\n  ]\n}\n')),(0,s.kt)("h3",{id:"chat-prompt-templates-manage-prompts-for-chat-models"},"Chat Prompt Templates: Manage Prompts for Chat Models"),(0,s.kt)("p",null,"You can make use of templating by using a ",(0,s.kt)("inlineCode",{parentName:"p"},"MessagePromptTemplate"),". You can build a ",(0,s.kt)("inlineCode",{parentName:"p"},"ChatPromptTemplate")," from one or more ",(0,s.kt)("inlineCode",{parentName:"p"},"MessagePromptTemplates"),". You can use ",(0,s.kt)("inlineCode",{parentName:"p"},"ChatPromptTemplate"),"'s ",(0,s.kt)("inlineCode",{parentName:"p"},"formatPromptValue")," -- this returns a ",(0,s.kt)("inlineCode",{parentName:"p"},"PromptValue"),", which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model."),(0,s.kt)("p",null,"Continuing with the previous example:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'import {\n  SystemMessagePromptTemplate,\n  HumanMessagePromptTemplate,\n  ChatPromptTemplate,\n} from "langchain/prompts";\n')),(0,s.kt)("p",null,"First we create a reusable template:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'const translationPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    "You are a helpful assistant that translates {input_language} to {output_language}."\n  ),\n  HumanMessagePromptTemplate.fromTemplate("{text}"),\n]);\n')),(0,s.kt)("p",null,"Then we can use the template to generate a response:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'const responseA = await chat.generatePrompt([\n  await translationPrompt.formatPromptValue({\n    input_language: "English",\n    output_language: "French",\n    text: "I love programming.",\n  }),\n]);\n\nconsole.log(responseA);\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'{\n  generations: [\n    [\n      {\n        text: "J\'aime programmer.",\n        message: AIChatMessage { text: "J\'aime programmer." }\n      }\n    ]\n  ]\n}\n')),(0,s.kt)("h3",{id:"model--prompt--llmchain"},"Model + Prompt = LLMChain"),(0,s.kt)("p",null,"This pattern of asking for the completion of a formatted prompt is quite common, so we introduce the next piece of the puzzle: LLMChain"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},"const chain = new LLMChain({\n  prompt: chatPrompt,\n  llm: chat,\n});\n")),(0,s.kt)("p",null,"Then you can call the chain:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'const responseB = await chain.call({\n  input_language: "English",\n  output_language: "French",\n  text: "I love programming.",\n});\n\nconsole.log(responseB);\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'{ text: "J\'aime programmer." }\n')),(0,s.kt)("p",null,"The chain will internally accumulate the messages sent to the model, and the ones received as output. Then it will inject the messages into the prompt on the next call. So you can call the chain a few times, and it remembers previous messages:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'const responseD = await chain.call({\n  input: "hi from London, how are you doing today",\n});\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"{\n  response: \"Hello! As an AI language model, I don't have feelings, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\"\n}\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'const responseE = await chain.call({\n  input: "Do you know where I am?",\n});\n\nconsole.log(responseE);\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'{\n  response: "Yes, you mentioned that you are from London. However, as an AI language model, I don\'t have access to your current location unless you provide me with that information."\n}\n')),(0,s.kt)("h3",{id:"agents-dynamically-run-chains-based-on-user-input"},"Agents: Dynamically Run Chains Based on User Input"),(0,s.kt)("p",null,"Finally, we introduce Tools and Agents, which extend the model with other abilities, such as search, or a calculator."),(0,s.kt)("p",null,"A tool is a function that takes a string (such as a search query) and returns a string (such as a search result). They also have a name and description, which are used by the chat model to identify which tool it should call."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},"class Tool {\n  name: string;\n  description: string;\n  call(arg: string): Promise<string>;\n}\n")),(0,s.kt)("p",null,"An agent is a stateless wrapper around an agent prompt chain (such as MRKL) which takes care of formatting tools into the prompt, as well as parsing the responses obtained from the chat model."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},"interface AgentStep {\n  action: AgentAction;\n  observation: string;\n}\n\ninterface AgentAction {\n  tool: string; // Tool.name\n  toolInput: string; // Tool.call argument\n}\n\ninterface AgentFinish {\n  returnValues: object;\n}\n\nclass Agent {\n  plan(steps: AgentStep[], inputs: object): Promise<AgentAction | AgentFinish>;\n}\n")),(0,s.kt)("p",null,"To make agents more powerful we need to make them iterative, ie. call the model multiple times until they arrive at the final answer. That's the job of the AgentExecutor."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},"class AgentExecutor {\n  // a simplified implementation\n  run(inputs: object) {\n    const steps = [];\n    while (true) {\n      const step = await this.agent.plan(steps, inputs);\n      if (step instanceof AgentFinish) {\n        return step.returnValues;\n      }\n      steps.push(step);\n    }\n  }\n}\n")),(0,s.kt)("p",null,"And finally, we can use the AgentExecutor to run an agent:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'// Define the list of tools the agent can use\nconst tools = [new SerpAPI()];\n// Create the agent from the chat model and the tools\nconst agent = ChatAgent.fromLLMAndTools(new ChatOpenAI(), tools);\n// Create an executor, which calls to the agent until an answer is found\nconst executor = AgentExecutor.fromAgentAndTools({ agent, tools });\n\nconst responseG = await executor.run(\n  "How many people live in canada as of 2023?"\n);\n\nconsole.log(responseG);\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"38,626,704.\n")),(0,s.kt)("h3",{id:"memory-add-state-to-chains-and-agents"},"Memory: Add State to Chains and Agents"),(0,s.kt)("p",null,"You can also use the chain to store state. This is useful for eg. chatbots, where you want to keep track of the conversation history. MessagesPlaceholder is a special prompt template that will be replaced with the messages passed in each call."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."\n  ),\n  new MessagesPlaceholder("history"),\n  HumanMessagePromptTemplate.fromTemplate("{input}"),\n]);\n\nconst chain = new ConversationChain({\n  memory: new BufferMemory({ returnMessages: true, memoryKey: "history" }),\n  prompt: chatPrompt,\n  llm: chat,\n});\n')),(0,s.kt)("h2",{id:"streaming"},"Streaming"),(0,s.kt)("p",null,"You can also use the streaming API to get words streamed back to you as they are generated. This is useful for eg. chatbots, where you want to show the user what is being generated as it is being generated. Note: OpenAI as of this writing does not support ",(0,s.kt)("inlineCode",{parentName:"p"},"tokenUsage")," reporting while streaming is enabled."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-typescript"},'const chatStreaming = new ChatOpenAI({\n  streaming: true,\n  callbackManager: CallbackManager.fromHandlers({\n    handleLLMNewToken(token: string) {\n      console.log(token);\n    },\n  },\n});\n\nconst responseD = await chatStreaming.call([\n  new HumanChatMessage("Write me a song about sparkling water."),\n]);\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Verse 1:\nBubbles in the bottle,\nLight and refreshing,\nIt's the drink that I love,\nMy thirst quenching blessing.\n\nChorus:\nSparkling water, my fountain of youth,\nI can't get enough, it's the perfect truth,\nIt's fizzy and fun, and oh so clear,\nSparkling water, it's crystal clear.\n\nVerse 2:\nNo calories or sugars,\nJust a burst of delight,\nIt's the perfect cooler,\nOn a hot summer night.\n\nChorus:\nSparkling water, my fountain of youth,\nI can't get enough, it's the perfect truth,\nIt's fizzy and fun, and oh so clear,\nSparkling water, it's crystal clear.\n\nBridge:\nIt's my happy place,\nIn every situation,\nMy daily dose of hydration,\nAlways bringing satisfaction.\n\nChorus:\nSparkling water, my fountain of youth,\nI can't get enough, it's the perfect truth,\nIt's fizzy and fun, and oh so clear,\nSparkling water, it's crystal clear.\n\nOutro:\nSparkling water, it's crystal clear,\nMy love for you will never disappear.\n")))}u.isMDXComponent=!0}}]);