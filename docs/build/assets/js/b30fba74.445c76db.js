"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5771],{6638:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>h,default:()=>f,frontMatter:()=>m,metadata:()=>c,toc:()=>u});var a=t(5773),o=(t(7378),t(5318)),s=t(6538);const i='import { ChatOpenAI } from "langchain/chat_models";\nimport { HumanChatMessage, SystemChatMessage } from "langchain/schema";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });\n  // Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.\n  const responseA = await chat.call([\n    new HumanChatMessage(\n      "What is a good name for a company that makes colorful socks?"\n    ),\n  ]);\n  console.log(responseA);\n  // AIChatMessage { text: \'\\n\\nRainbow Sox Co.\' }\n\n  // You can also pass in multiple messages to start a conversation.\n  // The first message is a system message that describes the context of the conversation.\n  // The second message is a human message that starts the conversation.\n  const responseB = await chat.call([\n    new SystemChatMessage(\n      "You are a helpful assistant that translates English to French."\n    ),\n    new HumanChatMessage("Translate: I love programming."),\n  ]);\n  console.log(responseB);\n  // AIChatMessage { text: "J\'aime programmer." }\n\n  // Similar to LLMs, you can also use `generate` to generate chat completions for multiple sets of messages.\n  const responseC = await chat.generate([\n    [\n      new SystemChatMessage(\n        "You are a helpful assistant that translates English to French."\n      ),\n      new HumanChatMessage(\n        "Translate this sentence from English to French. I love programming."\n      ),\n    ],\n    [\n      new SystemChatMessage(\n        "You are a helpful assistant that translates English to French."\n      ),\n      new HumanChatMessage(\n        "Translate this sentence from English to French. I love artificial intelligence."\n      ),\n    ],\n  ]);\n  console.log(responseC);\n  /*\n  {\n    generations: [\n      [\n        {\n          text: "J\'aime programmer.",\n          message: AIChatMessage { text: "J\'aime programmer." },\n        }\n      ],\n      [\n        {\n          text: "J\'aime l\'intelligence artificielle.",\n          message: AIChatMessage { text: "J\'aime l\'intelligence artificielle." }\n        }\n      ]\n    ]\n  }\n  */\n};\n',l="import { CallbackManager } from \"langchain/callbacks\";\nimport { ChatOpenAI } from \"langchain/chat_models\";\nimport { HumanChatMessage } from \"langchain/schema\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({\n    maxTokens: 25,\n    streaming: true,\n    callbackManager: CallbackManager.fromHandlers({\n      async handleLLMNewToken(token: string) {\n        console.log({ token });\n      },\n    }),\n  });\n\n  const response = await chat.call([new HumanChatMessage(\"Tell me a joke.\")]);\n\n  console.log(response);\n  // { token: '' }\n  // { token: '\\n\\n' }\n  // { token: 'Why' }\n  // { token: ' don' }\n  // { token: \"'t\" }\n  // { token: ' scientists' }\n  // { token: ' trust' }\n  // { token: ' atoms' }\n  // { token: '?\\n\\n' }\n  // { token: 'Because' }\n  // { token: ' they' }\n  // { token: ' make' }\n  // { token: ' up' }\n  // { token: ' everything' }\n  // { token: '.' }\n  // { token: '' }\n  // AIChatMessage {\n  //   text: \"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything.\"\n  // }\n};\n",r='import { ChatOpenAI } from "langchain/chat_models";\nimport { HumanChatMessage } from "langchain/schema";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI(\n    { temperature: 1, timeout: 1000 } // 1s timeout\n  );\n\n  const response = await chat.call([\n    new HumanChatMessage(\n      "What is a good name for a company that makes colorful socks?"\n    ),\n  ]);\n  console.log(response);\n  // AIChatMessage { text: \'\\n\\nRainbow Sox Co.\' }\n};\n',m={sidebar_label:"Additional Functionality"},h="Additional Functionality: Chat Models",c={unversionedId:"modules/models/chat/additional_functionality",id:"modules/models/chat/additional_functionality",title:"Additional Functionality: Chat Models",description:"We offer a number of additional features for chat models. In the examples below, we'll be using the ChatOpenAI model.",source:"@site/docs/modules/models/chat/additional_functionality.mdx",sourceDirName:"modules/models/chat",slug:"/modules/models/chat/additional_functionality",permalink:"/langchainjs/docs/modules/models/chat/additional_functionality",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/docs/modules/models/chat/additional_functionality.mdx",tags:[],version:"current",frontMatter:{sidebar_label:"Additional Functionality"},sidebar:"sidebar",previous:{title:"Integrations",permalink:"/langchainjs/docs/modules/models/chat/integrations"},next:{title:"Embeddings",permalink:"/langchainjs/docs/modules/models/embeddings/"}},d={},u=[{value:"Additional Methods",id:"additional-methods",level:2},{value:"Streaming",id:"streaming",level:2},{value:"Adding a timeout",id:"adding-a-timeout",level:2},{value:"Dealing with Rate Limits",id:"dealing-with-rate-limits",level:2},{value:"Dealing with API Errors",id:"dealing-with-api-errors",level:2}],p={toc:u},g="wrapper";function f(e){let{components:n,...t}=e;return(0,o.kt)(g,(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"additional-functionality-chat-models"},"Additional Functionality: Chat Models"),(0,o.kt)("p",null,"We offer a number of additional features for chat models. In the examples below, we'll be using the ",(0,o.kt)("inlineCode",{parentName:"p"},"ChatOpenAI")," model."),(0,o.kt)("h2",{id:"additional-methods"},"Additional Methods"),(0,o.kt)("p",null,"LangChain provides a number of additional methods for interacting with chat models:"),(0,o.kt)(s.Z,{language:"typescript",mdxType:"CodeBlock"},i),(0,o.kt)("h2",{id:"streaming"},"Streaming"),(0,o.kt)("p",null,"Similar to LLMs, you can stream responses from a chat model. This is useful for chatbots that need to respond to user input in real-time."),(0,o.kt)(s.Z,{language:"typescript",mdxType:"CodeBlock"},l),(0,o.kt)("h2",{id:"adding-a-timeout"},"Adding a timeout"),(0,o.kt)("p",null,"By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a ",(0,o.kt)("inlineCode",{parentName:"p"},"timeout")," option, in milliseconds, when you instantiate the model. For example, for OpenAI:"),(0,o.kt)(s.Z,{language:"typescript",mdxType:"CodeBlock"},r),(0,o.kt)("p",null,"Cureently, the timeout option is only supported for OpenAI models."),(0,o.kt)("h2",{id:"dealing-with-rate-limits"},"Dealing with Rate Limits"),(0,o.kt)("p",null,"Some providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a ",(0,o.kt)("inlineCode",{parentName:"p"},"maxConcurrency")," option when instantiating a Chat Model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete."),(0,o.kt)("p",null,"For example, if you set ",(0,o.kt)("inlineCode",{parentName:"p"},"maxConcurrency: 5"),", then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent."),(0,o.kt)("p",null,"To use this feature, simply pass ",(0,o.kt)("inlineCode",{parentName:"p"},"maxConcurrency: <number>")," when you instantiate the LLM. For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-typescript"},'import { ChatOpenAI } from "langchain/chat_models";\n\nconst model = new ChatOpenAI({ maxConcurrency: 5 });\n')),(0,o.kt)("h2",{id:"dealing-with-api-errors"},"Dealing with API Errors"),(0,o.kt)("p",null,"If the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a ",(0,o.kt)("inlineCode",{parentName:"p"},"maxRetries")," option when you instantiate the model. For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-typescript"},'import { ChatOpenAI } from "langchain/chat_models";\n\nconst model = new ChatOpenAI({ maxRetries: 10 });\n')))}f.isMDXComponent=!0}}]);