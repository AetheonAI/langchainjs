"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3742],{5162:(e,t,n)=>{n.d(t,{Z:()=>f});var a=n(7378),o=n(8944),s=n(5161),r=n(1884),m=n(5626),p=n(9213);const l={cardContainer:"cardContainer_H47c",cardTitle:"cardTitle_tTnA",cardDescription:"cardDescription_rTl4"};function i(e){let{href:t,children:n}=e;return a.createElement(r.Z,{href:t,className:(0,o.Z)("card padding--lg",l.cardContainer)},n)}function c(e){let{href:t,icon:n,title:s,description:r}=e;return a.createElement(i,{href:t},a.createElement("h2",{className:(0,o.Z)("text--truncate",l.cardTitle),title:s},n," ",s),r&&a.createElement("p",{className:(0,o.Z)("text--truncate",l.cardDescription),title:r},r))}function d(e){let{item:t}=e;const n=(0,s.Wl)(t);return n?a.createElement(c,{href:n,icon:"\ud83d\uddc3\ufe0f",title:t.label,description:(0,p.I)({message:"{count} items",id:"theme.docs.DocCard.categoryDescription",description:"The default description for a category card in the generated index about how many items this category includes"},{count:t.items.length})}):null}function u(e){let{item:t}=e;const n=(0,m.Z)(t.href)?"\ud83d\udcc4\ufe0f":"\ud83d\udd17",o=(0,s.xz)(t.docId??void 0);return a.createElement(c,{href:t.href,icon:n,title:t.label,description:o?.description})}function h(e){let{item:t}=e;switch(t.type){case"link":return a.createElement(u,{item:t});case"category":return a.createElement(d,{item:t});default:throw new Error(`unknown item type ${JSON.stringify(t)}`)}}function g(e){let{className:t}=e;const n=(0,s.jA)();return a.createElement(f,{items:n.items,className:t})}function f(e){const{items:t,className:n}=e;if(!t)return a.createElement(g,e);const r=(0,s.MN)(t);return a.createElement("section",{className:(0,o.Z)("row",n)},r.map(((e,t)=>a.createElement("article",{key:t,className:"col col--6 margin-bottom--lg"},a.createElement(h,{item:e})))))}},4252:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>g,frontMatter:()=>p,metadata:()=>i,toc:()=>d});var a=n(5773),o=(n(7378),n(5318)),s=n(6538);const r='import {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  PromptTemplate,\n  SystemMessagePromptTemplate,\n} from "langchain/prompts";\n\nexport const run = async () => {\n  // A `PromptTemplate` consists of a template string and a list of input variables.\n  const template = "What is a good name for a company that makes {product}?";\n  const promptA = new PromptTemplate({ template, inputVariables: ["product"] });\n\n  // We can use the `format` method to format the template with the given input values.\n  const responseA = await promptA.format({ product: "colorful socks" });\n  console.log({ responseA });\n  /*\n  {\n    responseA: \'What is a good name for a company that makes colorful socks?\'\n  }\n  */\n\n  // We can also use the `fromTemplate` method to create a `PromptTemplate` object.\n  const promptB = PromptTemplate.fromTemplate(\n    "What is a good name for a company that makes {product}?"\n  );\n  const responseB = await promptB.format({ product: "colorful socks" });\n  console.log({ responseB });\n  /*\n  {\n    responseB: \'What is a good name for a company that makes colorful socks?\'\n  }\n  */\n\n  // For chat models, we provide a `ChatPromptTemplate` class that can be used to format chat prompts.\n  const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      "You are a helpful assistant that translates {input_language} to {output_language}."\n    ),\n    HumanMessagePromptTemplate.fromTemplate("{text}"),\n  ]);\n\n  // The result can be formatted as a string using the `format` method.\n  const responseC = await chatPrompt.format({\n    input_language: "English",\n    output_language: "French",\n    text: "I love programming.",\n  });\n  console.log({ responseC });\n  /*\n  {\n    responseC: \'[{"text":"You are a helpful assistant that translates English to French."},{"text":"I love programming."}]\'\n  }\n  */\n\n  // The result can also be formatted as a list of `ChatMessage` objects by returning a `PromptValue` object and calling the `toChatMessages` method.\n  // More on this below.\n  const responseD = await chatPrompt.formatPromptValue({\n    input_language: "English",\n    output_language: "French",\n    text: "I love programming.",\n  });\n  const messages = responseD.toChatMessages();\n  console.log({ messages });\n  /*\n  {\n    messages: [\n        SystemChatMessage {\n          text: \'You are a helpful assistant that translates English to French.\'\n        },\n        HumanChatMessage { text: \'I love programming.\' }\n      ]\n  }\n  */\n};\n';var m=n(5162);const p={hide_table_of_contents:!0,sidebar_label:"Prompt Templates",sidebar_position:1},l="Prompt Templates",i={unversionedId:"modules/prompts/prompt_templates/index",id:"modules/prompts/prompt_templates/index",title:"Prompt Templates",description:"Conceptual Guide",source:"@site/docs/modules/prompts/prompt_templates/index.mdx",sourceDirName:"modules/prompts/prompt_templates",slug:"/modules/prompts/prompt_templates/",permalink:"/langchainjs/docs/modules/prompts/prompt_templates/",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/docs/modules/prompts/prompt_templates/index.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{hide_table_of_contents:!0,sidebar_label:"Prompt Templates",sidebar_position:1},sidebar:"sidebar",previous:{title:"Prompts",permalink:"/langchainjs/docs/modules/prompts/"},next:{title:"Additional Functionality",permalink:"/langchainjs/docs/modules/prompts/prompt_templates/additional_functionality"}},c={},d=[{value:"Dig deeper",id:"dig-deeper",level:2}],u={toc:d},h="wrapper";function g(e){let{components:t,...n}=e;return(0,o.kt)(h,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"prompt-templates"},"Prompt Templates"),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},(0,o.kt)("a",{parentName:"p",href:"https://docs.langchain.com/docs/components/prompts/prompt-template"},"Conceptual Guide"))),(0,o.kt)("p",null,"A ",(0,o.kt)("inlineCode",{parentName:"p"},"PromptTemplate")," allows you to make use of templating to generate a prompt. This is useful for when you want to use the same prompt outline in multiple places, but with certain values changed.\nPrompt templates are supported for both LLMs and chat models, as shown below:"),(0,o.kt)(s.Z,{language:"typescript",mdxType:"CodeBlock"},r),(0,o.kt)("h2",{id:"dig-deeper"},"Dig deeper"),(0,o.kt)(m.Z,{mdxType:"DocCardList"}))}g.isMDXComponent=!0}}]);