"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7667],{9798:(e,n,t)=>{t.d(n,{Z:()=>r});var a=t(7378),o=t(8944);const l={tabItem:"tabItem_wHwb"};function r(e){let{children:n,hidden:t,className:r}=e;return a.createElement("div",{role:"tabpanel",className:(0,o.Z)(l.tabItem,r),hidden:t},n)}},3930:(e,n,t)=>{t.d(n,{Z:()=>v});var a=t(5773),o=t(7378),l=t(8944),r=t(3457),s=t(3620),i=t(654),u=t(784),c=t(1819);function m(e){return function(e){return o.Children.map(e,(e=>{if((0,o.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))}(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:o}}=e;return{value:n,label:t,attributes:a,default:o}}))}function d(e){const{values:n,children:t}=e;return(0,o.useMemo)((()=>{const e=n??m(t);return function(e){const n=(0,u.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:t}=e;const a=(0,s.k6)(),l=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,i._X)(l),(0,o.useCallback)((e=>{if(!l)return;const n=new URLSearchParams(a.location.search);n.set(l,e),a.replace({...a.location,search:n.toString()})}),[l,a])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,l=d(e),[r,s]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:l}))),[i,u]=h({queryString:t,groupId:a}),[m,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,l]=(0,c.Nk)(t);return[a,(0,o.useCallback)((e=>{t&&l.set(e)}),[t,l])]}({groupId:a}),f=(()=>{const e=i??m;return p({value:e,tabValues:l})?e:null})();(0,o.useLayoutEffect)((()=>{f&&s(f)}),[f]);return{selectedValue:r,selectValue:(0,o.useCallback)((e=>{if(!p({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);s(e),u(e),g(e)}),[u,g,l]),tabValues:l}}var f=t(6457);const k={tabList:"tabList_J5MA",tabItem:"tabItem_l0OV"};function y(e){let{className:n,block:t,selectedValue:s,selectValue:i,tabValues:u}=e;const c=[],{blockElementScrollPositionUntilNextRender:m}=(0,r.o5)(),d=e=>{const n=e.currentTarget,t=c.indexOf(n),a=u[t].value;a!==s&&(m(n),i(a))},p=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=c.indexOf(e.currentTarget)+1;n=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(e.currentTarget)-1;n=c[t]??c[c.length-1];break}}n?.focus()};return o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.Z)("tabs",{"tabs--block":t},n)},u.map((e=>{let{value:n,label:t,attributes:r}=e;return o.createElement("li",(0,a.Z)({role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,key:n,ref:e=>c.push(e),onKeyDown:p,onClick:d},r,{className:(0,l.Z)("tabs__item",k.tabItem,r?.className,{"tabs__item--active":s===n})}),t??n)})))}function b(e){let{lazy:n,children:t,selectedValue:a}=e;if(t=Array.isArray(t)?t:[t],n){const e=t.find((e=>e.props.value===a));return e?(0,o.cloneElement)(e,{className:"margin-top--md"}):null}return o.createElement("div",{className:"margin-top--md"},t.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==a}))))}function w(e){const n=g(e);return o.createElement("div",{className:(0,l.Z)("tabs-container",k.tabList)},o.createElement(y,(0,a.Z)({},e,n)),o.createElement(b,(0,a.Z)({},e,n)))}function v(e){const n=(0,f.Z)();return o.createElement(w,(0,a.Z)({key:String(n)},e))}},3379:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>g,contentTitle:()=>p,default:()=>b,frontMatter:()=>d,metadata:()=>h,toc:()=>f});var a=t(5773),o=(t(7378),t(5318)),l=t(3930),r=t(9798),s=t(6538);const i='import { OpenAI } from "langchain/llms";\n\nexport const run = async () => {\n  const modelA = new OpenAI();\n  // `call` is a simple string-in, string-out method for interacting with the model.\n  const resA = await modelA.call(\n    "What would be a good company name a company that makes colorful socks?"\n  );\n  console.log({ resA });\n  // { resA: \'\\n\\nSocktastic Colors\' }\n\n  // `generate` allows you to generate multiple completions for multiple prompts (in a single request for some models).\n  const resB = await modelA.generate([\n    "What would be a good company name a company that makes colorful socks?",\n    "What would be a good company name a company that makes colorful sweaters?",\n  ]);\n\n  // `resB` is a `LLMResult` object with a `generations` field and `llmOutput` field.\n  // `generations` is a `Generation[][]`, each `Generation` having a `text` field.\n  // Each input to the LLM could have multiple generations (depending on the `n` parameter), hence the list of lists.\n  console.log(JSON.stringify(resB, null, 2));\n  /*\n  {\n      "generations": [\n          [{\n              "text": "\\n\\nVibrant Socks Co.",\n              "generationInfo": {\n                  "finishReason": "stop",\n                  "logprobs": null\n              }\n          }],\n          [{\n              "text": "\\n\\nRainbow Knitworks.",\n              "generationInfo": {\n                  "finishReason": "stop",\n                  "logprobs": null\n              }\n          }]\n      ],\n      "llmOutput": {\n          "tokenUsage": {\n              "completionTokens": 17,\n              "promptTokens": 29,\n              "totalTokens": 46\n          }\n      }\n  }\n  */\n\n  // We can specify additional parameters the specific model provider supports, like `temperature`:\n  const modelB = new OpenAI({ temperature: 0.9 });\n  const resC = await modelA.call(\n    "What would be a good company name a company that makes colorful socks?"\n  );\n  console.log({ resC });\n  // { resC: \'\\n\\nKaleidoSox\' }\n\n  // We can get the number of tokens for a given input for a specific model.\n  const numTokens = modelB.getNumTokens("How many tokens are in this input?");\n  console.log({ numTokens });\n  // { numTokens: 8 }\n};\n',u='import { LLMResult } from "langchain/schema";\nimport { CallbackManager } from "langchain/callbacks";\nimport { OpenAI } from "langchain/llms";\n\nexport const run = async () => {\n  // We can pass in a `CallbackManager` to the LLM constructor to get callbacks for various events.\n  const callbackManager = CallbackManager.fromHandlers({\n    handleLLMStart: async (llm: { name: string }, prompts: string[]) => {\n      console.log(JSON.stringify(llm, null, 2));\n      console.log(JSON.stringify(prompts, null, 2));\n    },\n    handleLLMEnd: async (output: LLMResult) => {\n      console.log(JSON.stringify(output, null, 2));\n    },\n    handleLLMError: async (err: Error) => {\n      console.error(err);\n    },\n  });\n\n  const model = new OpenAI({\n    verbose: true,\n    callbackManager,\n  });\n\n  await model.call(\n    "What would be a good company name a company that makes colorful socks?"\n  );\n  // {\n  //     "name": "openai"\n  // }\n  // [\n  //     "What would be a good company name a company that makes colorful socks?"\n  // ]\n  // {\n  //   "generations": [\n  //     [\n  //         {\n  //             "text": "\\n\\nSocktastic Splashes.",\n  //             "generationInfo": {\n  //                 "finishReason": "stop",\n  //                 "logprobs": null\n  //             }\n  //         }\n  //     ]\n  //  ],\n  //   "llmOutput": {\n  //     "tokenUsage": {\n  //         "completionTokens": 9,\n  //          "promptTokens": 14,\n  //          "totalTokens": 23\n  //     }\n  //   }\n  // }\n};\n',c="import { CallbackManager } from \"langchain/callbacks\";\nimport { OpenAI } from \"langchain/llms\";\n\nexport const run = async () => {\n  // To enable streaming, we pass in `streaming: true` to the LLM constructor.\n  // Additionally, we pass in a `CallbackManager` with a handler set up for the `handleLLMNewToken` event.\n  const chat = new OpenAI({\n    maxTokens: 25,\n    streaming: true,\n    callbackManager: CallbackManager.fromHandlers({\n      async handleLLMNewToken(token: string) {\n        console.log({ token });\n      },\n    }),\n  });\n\n  const response = await chat.call(\"Tell me a joke.\");\n  console.log(response);\n  /*\n  { token: '\\n' }\n  { token: '\\n' }\n  { token: 'Q' }\n  { token: ':' }\n  { token: ' Why' }\n  { token: ' did' }\n  { token: ' the' }\n  { token: ' chicken' }\n  { token: ' cross' }\n  { token: ' the' }\n  { token: ' playground' }\n  { token: '?' }\n  { token: '\\n' }\n  { token: 'A' }\n  { token: ':' }\n  { token: ' To' }\n  { token: ' get' }\n  { token: ' to' }\n  { token: ' the' }\n  { token: ' other' }\n  { token: ' slide' }\n  { token: '.' }\n\n\n  Q: Why did the chicken cross the playground?\n  A: To get to the other slide.\n  */\n};\n",m='import { OpenAI } from "langchain/llms";\n\nexport const run = async () => {\n  const model = new OpenAI(\n    { temperature: 1, timeout: 1000 } // 1s timeout\n  );\n\n  const resA = await model.call(\n    "What would be a good company name a company that makes colorful socks?"\n  );\n\n  console.log({ resA });\n  // \'\\n\\nSocktastic Colors\' }\n};\n',d={sidebar_label:"Additional Functionality"},p="Additional Functionality: LLMs",h={unversionedId:"modules/models/llms/additional_functionality",id:"modules/models/llms/additional_functionality",title:"Additional Functionality: LLMs",description:"We offer a number of additional features for LLMs. In most of the examples below, we'll be using the OpenAI LLM. However, all of these features are available for all LLMs.",source:"@site/docs/modules/models/llms/additional_functionality.mdx",sourceDirName:"modules/models/llms",slug:"/modules/models/llms/additional_functionality",permalink:"/langchainjs/docs/modules/models/llms/additional_functionality",draft:!1,editUrl:"https://github.com/hwchase17/langchainjs/docs/modules/models/llms/additional_functionality.mdx",tags:[],version:"current",frontMatter:{sidebar_label:"Additional Functionality"},sidebar:"sidebar",previous:{title:"Integrations",permalink:"/langchainjs/docs/modules/models/llms/integrations"},next:{title:"Prompts",permalink:"/langchainjs/docs/modules/prompts/"}},g={},f=[{value:"Additional Methods",id:"additional-methods",level:2},{value:"Streaming Responses",id:"streaming-responses",level:2},{value:"Caching",id:"caching",level:2},{value:"Caching in-memory",id:"caching-in-memory",level:3},{value:"Caching with Redis",id:"caching-with-redis",level:3},{value:"Adding a timeout",id:"adding-a-timeout",level:2},{value:"Dealing with Rate Limits",id:"dealing-with-rate-limits",level:2},{value:"Dealing with API Errors",id:"dealing-with-api-errors",level:2},{value:"Logging for Debugging",id:"logging-for-debugging",level:2}],k={toc:f},y="wrapper";function b(e){let{components:n,...t}=e;return(0,o.kt)(y,(0,a.Z)({},k,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"additional-functionality-llms"},"Additional Functionality: LLMs"),(0,o.kt)("p",null,"We offer a number of additional features for LLMs. In most of the examples below, we'll be using the ",(0,o.kt)("inlineCode",{parentName:"p"},"OpenAI")," LLM. However, all of these features are available for all LLMs."),(0,o.kt)("h2",{id:"additional-methods"},"Additional Methods"),(0,o.kt)("p",null,"LangChain provides a number of additional methods for interacting with LLMs:"),(0,o.kt)(s.Z,{language:"typescript",mdxType:"CodeBlock"},i),(0,o.kt)("h2",{id:"streaming-responses"},"Streaming Responses"),(0,o.kt)("p",null,"Some LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.\nLangChain currently provides streaming for the ",(0,o.kt)("inlineCode",{parentName:"p"},"OpenAI")," LLM:"),(0,o.kt)(s.Z,{language:"typescript",mdxType:"CodeBlock"},c),(0,o.kt)("h2",{id:"caching"},"Caching"),(0,o.kt)("p",null,"LangChain provides an optional caching layer for LLMs. This is useful for two reasons:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times."),(0,o.kt)("li",{parentName:"ol"},"It can speed up your application by reducing the number of API calls you make to the LLM provider.")),(0,o.kt)("h3",{id:"caching-in-memory"},"Caching in-memory"),(0,o.kt)("p",null,"The default cache is stored in-memory. This means that if you restart your application, the cache will be cleared."),(0,o.kt)("p",null,"To enable it you can pass ",(0,o.kt)("inlineCode",{parentName:"p"},"cache: true")," when you instantiate the LLM. For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-typescript"},'import { OpenAI } from "langchain/llms";\n\nconst model = new OpenAI({ cache: true });\n')),(0,o.kt)("h3",{id:"caching-with-redis"},"Caching with Redis"),(0,o.kt)("p",null,"LangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers. To use it, you'll need to install the ",(0,o.kt)("inlineCode",{parentName:"p"},"redis")," package:"),(0,o.kt)(l.Z,{groupId:"npm2yarn",mdxType:"Tabs"},(0,o.kt)(r.Z,{value:"npm",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"npm install redis\n"))),(0,o.kt)(r.Z,{value:"yarn",label:"Yarn",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"yarn add redis\n")))),(0,o.kt)("p",null,"Then, you can pass a ",(0,o.kt)("inlineCode",{parentName:"p"},"cache")," option when you instantiate the LLM. For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-typescript"},'import { OpenAI } from "langchain/llms";\nimport { RedisCache } from "langchain/cache";\nimport { createClient } from "redis";\n\n// See https://github.com/redis/node-redis for connection options\nconst client = createClient();\nconst cache = new RedisCache(client);\n\nconst model = new OpenAI({ cache });\n')),(0,o.kt)("h2",{id:"adding-a-timeout"},"Adding a timeout"),(0,o.kt)("p",null,"By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a ",(0,o.kt)("inlineCode",{parentName:"p"},"timeout")," option, in milliseconds, when you instantiate the model. For example, for OpenAI:"),(0,o.kt)(s.Z,{language:"typescript",mdxType:"CodeBlock"},m),(0,o.kt)("p",null,"Cureently, the timeout option is only supported for OpenAI models."),(0,o.kt)("h2",{id:"dealing-with-rate-limits"},"Dealing with Rate Limits"),(0,o.kt)("p",null,"Some LLM providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a ",(0,o.kt)("inlineCode",{parentName:"p"},"maxConcurrency")," option when instantiating an LLM. This option allows you to specify the maximum number of concurrent requests you want to make to the LLM provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete."),(0,o.kt)("p",null,"For example, if you set ",(0,o.kt)("inlineCode",{parentName:"p"},"maxConcurrency: 5"),", then LangChain will only send 5 requests to the LLM provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent."),(0,o.kt)("p",null,"To use this feature, simply pass ",(0,o.kt)("inlineCode",{parentName:"p"},"maxConcurrency: <number>")," when you instantiate the LLM. For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-typescript"},'import { OpenAI } from "langchain/llms";\n\nconst model = new OpenAI({ maxConcurrency: 5 });\n')),(0,o.kt)("h2",{id:"dealing-with-api-errors"},"Dealing with API Errors"),(0,o.kt)("p",null,"If the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a ",(0,o.kt)("inlineCode",{parentName:"p"},"maxRetries")," option when you instantiate the model. For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-typescript"},'import { OpenAI } from "langchain/llms";\n\nconst model = new OpenAI({ maxRetries: 10 });\n')),(0,o.kt)("h2",{id:"logging-for-debugging"},"Logging for Debugging"),(0,o.kt)("p",null,"Especially when using an agent, there can be a lot of back-and-forth going on behind the scenes as a LLM processes a chain. For agents, the response object contains an intermediateSteps object that you can print to see an overview of the steps it took to get there. If that's not enough and you want to see every exchange with the LLM, you can use the LLMCallbackManager to write yourself custom logging (or anything else you want to do) as the model goes through the steps:"),(0,o.kt)(s.Z,{language:"typescript",mdxType:"CodeBlock"},u))}b.isMDXComponent=!0}}]);